{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into chunks to maintain contextual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T18:44:39.459185Z",
     "iopub.status.busy": "2025-01-04T18:44:39.458851Z",
     "iopub.status.idle": "2025-01-04T18:45:00.471317Z",
     "shell.execute_reply": "2025-01-04T18:45:00.470601Z",
     "shell.execute_reply.started": "2025-01-04T18:44:39.459156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load text data\n",
    "file_path = 'combined_text.txt'\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Chunk the data (e.g., by word count)\n",
    "def chunk_text(text, chunk_size=100):  # Chunk size in words\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(text, chunk_size=100)\n",
    "\n",
    "# Generate embeddings for the chunks\n",
    "chunk_embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "# Build FAISS index\n",
    "dimension = chunk_embeddings.shape[1]  # Embedding size\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 (Euclidean distance) index\n",
    "index.add(np.array(chunk_embeddings))  # Add chunk embeddings to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ipywidgets package to use notebook_login()\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d07c1009e4417cbcf512ec32cf5be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T18:45:18.893679Z",
     "iopub.status.busy": "2025-01-04T18:45:18.893398Z",
     "iopub.status.idle": "2025-01-04T18:45:27.116716Z",
     "shell.execute_reply": "2025-01-04T18:45:27.116066Z",
     "shell.execute_reply.started": "2025-01-04T18:45:18.893657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"  # Replace with your model name\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "if llama_tokenizer.pad_token_id is None:\n",
    "    llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id  # Use EOS token as padding token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T18:49:36.878134Z",
     "iopub.status.busy": "2025-01-04T18:49:36.877771Z",
     "iopub.status.idle": "2025-01-04T18:49:36.884390Z",
     "shell.execute_reply": "2025-01-04T18:49:36.883345Z",
     "shell.execute_reply.started": "2025-01-04T18:49:36.878106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the top 3 relevant chunks of data from the index\n",
    "def retrieve_relevant_chunks(query, index, chunks, embedding_model, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "    return [chunks[idx] for idx in indices[0]]\n",
    "\n",
    "def combine_chunks(chunks):\n",
    "    return \"\\n\".join(chunks)\n",
    "\n",
    "def generate_answer(context, query):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048, padding=True)\n",
    "    \n",
    "    outputs = llama_model.generate(\n",
    "        inputs[\"input_ids\"].to('cuda'),\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.5,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    \n",
    "    answer = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Answers from Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T18:49:38.729884Z",
     "iopub.status.busy": "2025-01-04T18:49:38.729546Z",
     "iopub.status.idle": "2025-01-04T18:49:38.763008Z",
     "shell.execute_reply": "2025-01-04T18:49:38.762124Z",
     "shell.execute_reply.started": "2025-01-04T18:49:38.729856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input(\"Enter your query: \")\n",
    "\n",
    "    #  Retrieve relevant chunks and combine them into one text\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query, index, chunks, embedding_model)\n",
    "    context = combine_chunks(retrieved_chunks)\n",
    "    \n",
    "    # Make the answer generation from LLM\n",
    "    answer = generate_answer(context, query)\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6424706,
     "sourceId": 10371976,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
